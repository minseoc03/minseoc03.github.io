---
layout: single
title:  "MLP & Non-Linearity"
date:   2024-04-24 21:10:54 
categories: [ml, dl_theory]
author_profile: false
sidebar:
  nav: "ml"
---
## 4-1. MLP & Non-Linear Activation

### MLP(Multi-Layer Perceptron)

- 이전에서도 말했듯이 MLP는 FC Layer를 기준으로 한다.
- 예시 MLP 구조
    - 입력 층의 노드 : 2개 ($x_1,x_2$)
    - 은닉 층의 갯수 : 1개
    - 은닉 층의 노드 갯수 : 3개 $(f_1, f_1, f_1)$
    - 출력 층의 노드 갯수 : 2개 $(f_2, f_2)$
- MLP를 벡터와 행렬로 표현
    - $\begin{bmatrix}x_1 & x_2\end{bmatrix}\begin{bmatrix}w_1&w_3&w_5 \\ w_2 & w_4 & w_6\end{bmatrix} + \begin{bmatrix}b_1&b_2&b_3\end{bmatrix}$
        - 이 식이 입력 층에서 은닉 층으로 가는 계산식이다.
    - $\vec f_1(\begin{bmatrix}x_1 & x_2\end{bmatrix}\begin{bmatrix}w_1&w_3&w_5 \\ w_2 & w_4 & w_6\end{bmatrix} + \begin{bmatrix}b_1&b_2&b_3\end{bmatrix})$
        - 그리고 이후에 은닉층에 있는 활성화 함수를 통과시켜준다.
    - $\vec f_1(\vec xW_1+\vec b_1)$
        - 입력 층의 노드를 $\vec x$라고 적는다.
        - 이후 첫번째 레이어에서 나오는 가중치와 편향 값을 각각 $W_1, \vec b_1$라고 해준다.
    - $\vec f_1(\vec xW_1+\vec b_1)W_2+\vec b_2$
        - 이후 두번째 레이어에 해당하는 가중치 행렬을 곱해주고 편향 값을 더해준다.
    - $\vec f_2(\vec f_1(\vec xW_1+\vec b_1)W_2+\vec b_2) = \text{Output}$
        - 마지막으로 출력층에 해당하는 활성화 함수를 통과시켜준다.
    
    <aside>
    💡
    
    이와 같이 MLP를 벡터와 행렬로 표현하는 방법은 가중치 행렬을 곱해주고 편향 벡터를 더해주고 활성화 함수를 통과시키는 과정의 반복이다.
    
    </aside>
    
    <aside>
    💡
    
    가중치 행렬의 크기는 (이전 층의 노드 갯수) x (다음 층의 노드 갯수)
    
    편향 벡터의 크기는 1 x (다음 층의 노드 갯수)
    
    </aside>
    

### Non-Linear Activation

- 위에서 설명한 예시대로 인공 신경망을 깊게 만들면 성능이 올라가나?
- 이에 대한 답은 Non-Linear Activation일때만이다.
- 그러면 왜 Linear Activation은 깊게 만들어도 성능이 증가하지 않는가?
    - $\vec f_2(\vec f_1(\vec xW_1+\vec b_1)W_2+\vec b_2) = (\vec xW_1+\vec b_1)W_2+b_2$
    - Linear Activation은 들어오는대로 똑같이 내보내기 때문에 이와 같은 식이 성립한다. 해당 식을 풀어보면…
    - $\vec xW_1W_2+\vec b_1W_2+\vec b_2$
        - $W_1W_2$는 2x2행렬이 되고 $\vec b_1W_2+\vec b_2$는 1x2 벡터가 되어버린다.
        - 즉, 단순 $\vec x W+\vec b$와 달라질게 없다. 
        오히려 찾아야 할 파라미터가 많아지기만 할 뿐이다.

<aside>
💡

인공 신경망이 아무리 깊어도 활성화 함수가 선형이라면 의미가 없다. 

다시 말하면 은닉 층이 없는 FC Layer 이하의 표현력만 가진다.

</aside>

<aside>
💡

결론은 Non-Linear Activation 함수는 매우 중요한 장치이다.

입력과 출력의 복잡한 비선형 관계를 표현할 수 있게 해준다.

</aside>