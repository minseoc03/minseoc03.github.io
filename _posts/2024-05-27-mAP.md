---
layout: single
title:  "mAP"
date:   2024-05-27 21:10:54 
categories: [ml, object_detection]
author_profile: false
sidebar:
  nav: "ml"
---

## mAP (median Average Precision)

mAP란 object detection에서 사용되는 평가지표로써 현재까지도 최신 논문에서 성능의 기준으로 쓰이는 평가지표이다.

mAP라는 개념을 이해하기 위해서는 Precision, Recall, 그리고 AP의 개념을 이해하고 있어야 가능하기 때문에 위 3가지에 대해 간단하게 설명해보도록 하겠다.

### Precision & Recall

(1) Precision (정밀도)

- 정밀도란, Positive라고 예측한 결과 중 정말로 Positive인 비율을 말한다.

(2) Recall (재현율)

- 재현율이란, 실제 값이 Positive인 결과 중 예측 값이 Positive인 비율을 말한다.

그리고 해당 지표들 같은 경우는 오차 행렬 (Confusion Matrix)를 통해 표현이 가능하다.

### Confusion Matrix

![image 6.png](/assets/images/object_detection/image%206.png)

- **TP (True Positive)**
    - 예측 값이 Positive인 결과 중에 실제 값도 Positive인 경우
- **FP (False Positive)**
    - 예측 값이 Positive 였지만 실제 값은 Negative인 경우
- **FN (False Negative)**
    - 예측 값이 Negative 였지만 실제 값은 Positive인 경우
- **TN (True Negative)**
    - 예측 값이 Negative인 결과 중에 실제 값도 Negative인 경우

**P와 N이 모델이 예측을 했는지 아닌지(BBox를 객체 주변에 만들었는지)**를 표현하고 **T와 F가 실제 값과 맞았는지 아닌지**를 표현한다고 생각하면 오차 행렬을 이해하기 더욱 쉬울 것이다.

오차 행렬 같은 경우 이진 분류에서 쓰이는 평가지표인데 해당 지표를 Classification과 Localization을 둘 다 하는 object detection에 적용할려면 어떻게 해야할까?

![image.png](/assets/images/object_detection/image%201%202.png)

**TP** - 예측 BBox와 실제 BBox의 IoU가 0.5 이상이고 클래스 예측도 정답과 같은 경우

**FP** - 예측 BBox를 만들어냈지만 (1) 클래스가 다르거나 (2) IoU가 0.5보다 작은 경우

**FN** - 예측 BBox를 만들어내야하는데 만들지 못했을 경우

### Precision & Recall in Confusion Matrix

(1) 정밀도

- 정밀도는 위에서 말했듯이 예측 값이 Positive인 경우 중 실제 값도 Positive인 경우를 말한다.
- 즉, 예측 값이 Positive인 TP와 FP에서 실제 값도 Positive인 TP를 말하는 것이다.

$$
Precision = \dfrac{TP}{TP+FP}
$$

(2) 재현율

- 재현율은 실제 값이 Positive인 경우에서 예측 값이 Positive인 경우를 말한다.
- 즉, 실제 값이 Positive인 TP와 FN에서 예측 값이 Positive인 TP를 말하는 것이다.

$$
Recall = \dfrac{TP}{TP+FN}
$$

### Precision & Recall Trade-Off

정밀도와 재현율 모두 중요한 평가지표이며 두 지표 모두 높을수록 성능이 좋다고 말할 수 있다. 
하지만 이 둘은 상호보완적 즉, 트레이드 오프가 존재하기에 **두 지표가 동시에 높아질수는 없다.**

정밀도와 재현율을 높이기 위해서는 Confidence Threshold 값을 변경을 해야한다.
NMS의 1번 과정은 Confidence 점수가 임계값보다 낮은 BBox들을 제거하는 것이었다.

즉, **임계값이 낮아진다면** 내가 원하는 객체를 담고 있을 확률이 낮은 BBox도 남기기 때문에 FP가 증가하고 FN은 낮아질 것이다. 그렇기에 **정밀도는 낮아지고 재현율이 높아질 것이다.**

반대로 **임계값이 높아진다면** 원하는 객체를 담고 있을 확률이 높은 BBox만 남기기에 FP는 줄어들고 FN이 증가할 것이다. 그렇기에 **정밀도는 높아지고 재현율은 낮아진다.**

![image.png](/assets/images/object_detection/image%202%201.png)

### Precision-Recall Curve

PR Curve란 **임계값을 조정하며 달라지는 Recall 값에 따라 변하는 Precision 값의 그래프**를 말한다.

예를 들어 임계값이 낮아지면 BBox들은 증가하고 Recall 값은 높아질 것이다. 하지만 그와 동시에 Precision 값은 줄어들기에 대부분의 PR Curve는 다음과 같은 양상을 띄운다.

![image.png](/assets/images/object_detection/image%203%201.png)

### AP (Average Precision)

여기서 AP란**, PR Curve의 면적을 뜻하는데 커브 아래의 넓이**를 말하는 것이다.

해당 넓이를 구할때에는 Right Riemann Sum 을 이용하여 오른쪽 최대 Precision 값을 기준으로 그래프를 직사각형 형태로 만든다.

![image.png](/assets/images/object_detection/image%204%201.png)

### mAP (median Average Precision)

위에서 말한 AP는 객체 하나에 해당하는 지표이다. Precision과 Recall 모두 이진 분류에서 쓰이는 지표이기에 각 객체 클래스마다 각각 다른 Precision과 Recall이 존재하므로 각 객체마다 AP가 따로 있을 것이다.

여기서 mAP는 이미지에 등장하는 객체 종류마다의 AP의 평균을 구한 것이다.

mAP는 수치가 클수록 성능이 좋다는 것을 의미하며 다음 그래프는 YOLO 논문에서 발췌한 것이다.

![image.png](/assets/images/object_detection/image%205%201.png)

다음과 같이 YOLO의 버전이 올라갈수록 mAP 그래프가 좌측 상단에 몰리는 것을 볼 수가 있는데 왼쪽 상단에 붙을수록 성능이 더 좋다는 것을 알 수 있다.