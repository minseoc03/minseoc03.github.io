---
layout: single
title:  "Multi-class Classification"
date:   2024-04-30 21:10:54 
categories: [ml, dl_theory]
author_profile: false
sidebar:
  nav: "ml"
---

## 5-5. 다중 분류 (Multi-Class Classification)

### 다중 분류 개념 설명

- 다중 분류란 이전의 예시와 같이 고양이 혹은 강아지로 분류하는게 아니라 분류할 항목이 3개 이상인 경우를 말한다.
- 즉, 이번 노트에서는 강아지, 고양이, 혹은 소 라는 3가지 항목으로 분류를 해야한다고 하자.
- 항목이 3가지이므로 각각의 항목일 확률을 출력하는 노드 3개가 필요하다.
    - 이진 분류에서는 1 아니면 0이기 때문에 출력 노드가 1개여도 됐지만 여기선 항목 갯수 만큼의 노드가 필요하다.
- 정답 라벨은 강아지라면 $\begin{bmatrix}1 & 0 & 0\end{bmatrix}$, 고양이라면 $\begin{bmatrix}0 & 1 & 0\end{bmatrix}$, 소라면 $\begin{bmatrix}0 & 0 & 1\end{bmatrix}$ 이렇게 표현한다.
    - 이는 원-핫 인코딩 기법이라고 부른다.

![image 21.png](/assets/images/dl-theory/image%2021.png)

- 사진이 3x100x100 사이즈라면 출력 노드가 3개니까 90,003개의 가중치 및 편향을 업데이트 해줘야한다.
- 활성화 함수는 softmax라는 것을 사용할 것인데 이는 다른 활성화 함수와 달리 입력을 항목 갯수만큼 받고 그 갯수만큼 출력한다.

### Softmax

- Softmax 함수를 통과 시키면 입력된 값들의 합이 1이 되도록 만들어주며 확률 값을 출력시킨다.
- 예를 들어서 입력이 $x_1,x_2,x_3$라고 가정하자…
    - Softmax ⇒ $\begin{bmatrix}\dfrac{e^{x_1}}{e^{x_1}+e^{x_2}+e^{x_3}} \\ \dfrac{e^{x_2}}{e^{x_1}+e^{x_2}+e^{x_3}} \\ \dfrac{e^{x_3}}{e^{x_1}+e^{x_2}+e^{x_3}}\end{bmatrix}$
    - 이런 식으로 입력을 바꿔주어 합이 1이 되도록 해준다.
- 그렇다면 다른 방법으로 합이 1로 만드는 건 왜 안될까?
    - $\dfrac{x_1}{x_1+x_2+x_3}$ 이런 형태로 만들면 안될까?
        - 만약 x값에 음수가 포함되어있으면 안된다.
    - 그렇다면 $\dfrac{\midx_1\mid}{\midx_1\mid+\midx_2\mid+\midx_3\mid}$ 이건?
        - 물론 합이 1로 나오긴 하지만 가중치 초기화를 할 때 0 근처로 하기 때문에 불안정하다.
        - 그리고 1,1,1 과 1,-1,-1을 같은 형태로 취급하기에 표현 방식이 제한된다.

### Multi-Label Classification

- 만약 softmax가 아니라 각각의 노드에 sigmoid를 적용하면 어떨까?
- 각 출력 노드의 합이 1이 되진 않지만 각각의 항목일 확률을 출력해준다.
- 이는 다중 분류가 아니라 multi-label classification에 어울리는 모델이다.
    - 예를 들어 사진에 개와 고양이 둘 다 있을 때 둘 다 판단하기 위한 문제에 어울린다.

### MLE의 관점에서 바라본 다중 분류

- 다중 분류에서는 멀티누이(multinouli) 분포 혹은 카테고리 분포를 가정하고 한다.
- 멀티누이는…
    - $\prod\limits_iq_i^{y_i}$ 라는 식으로 표현이 가능하다.
    - 베르누이에서는 $q_2 = 1-q_1$  그리고 $y_2 = 1-y_1$이라고 표현을 해서 식이 다르게 보이겠지만 사실 치환하지 않으면 같은 식이다.
- 이를 NLL로 표현하자면…
    - $L = -\log q_1^{y_1}q_2^{y_2}q_3^{y_3} = -y_1\log q_1 -y_2\log q_2 -y_3\log q_3$
    - 위와 같은 식을 우리는 교차 엔트로피 (Cross-Entropy)라고 부른다.
- 정보이론에서 배웠듯이 교차 엔트로피는 엔트로피보다 클수 밖에 없다.
    - $-y_1\log{y_1}-y_2\log{y_2}-y_3\log{y_3} \leq -y_1\log q_1 -y_2\log q_2 -y_3\log q_3$
    - 엔트로피의 값을 보자면…
        - 예를 들어 y_1 = 1이라고 해보자.
        - $-\log 1 = 0$ 이라는 값에 도달하기에 언제든 엔트로피는 0이다.
    - 즉, 교차 엔트로피의 값은 무조건 0보다 크다.
    - 교차 엔트로피를 손실 함수로 활용하여 q의 분포를 y의 분포와 동일하게 만드는 것이 다중분류에서의 최적화 방향이다.