---
layout: single
title:  "RNN"
date:   2024-05-14 20:10:54 
categories: [ml, dl_theory]
author_profile: false
sidebar:
  nav: "ml"
---

## 9-1. RNN 기본 개념

### RNN(Recurrent Neural Network)

- RNN은 순환 신경망이라고도 불리우며 연속형 데이터에 좋은 성능을 보여준다.
- RNN의 가장 큰 특징은 이전의 정보를 기억하며 다음 정보의 예측에 쓰인다는 것이다.

### RNN의 기본 구조

![image 6.png](/assets/images/dl-theory/image%206.png)

- 이와 같이 입력이 $x_1, x_2,x_3$가 있다고 가정 하자.
    - $x_1, x_2,x_3$는 연속형 데이터고 순서가 정해져있다.
    - 예를 들면 $x_1$은 ‘나는’, $x_2$는 ‘강사’, $x_3$는 ‘입니다’와 같이 문장을 이루는 단어는 순서가 있으므로 연속형 데이터이다.
- 이전의 MLP에서 $h_{t-1}$라는 정보를 가지고 와서 현재 $t$ 상태에 적용시킨다.

### RNN의 수식

![image.png](/assets/images/dl-theory/image%201%204.png)

- 위에 보여지는 식은 일반화 된 식이고 $t = 3$까지만 예를 들어 보여주겠다.

- $\vec h_1 = \tanh(x_1W_x+\vec b)$
- $\vec h_2 = \tanh (x_2W_x+\vec h_1W_h+\vec b)$
- $\vec h_3 = \tanh (x_3W_x+\vec h_2W_h+\vec b)$
- tanh는 RNN에서 성능이 잘 나오는 활성화 함수이며 시그모이드와 비슷하지만 값의 범위가 -1부터 1까지이다.
- 최대 기울기 1이므로 이 또한 vanishing gradient 문제가 있다.

![image.png](/assets/images/dl-theory/image%202%202.png)

- $\hat y_3 = \vec h_3W_y+\vec b_y$
    - 마지막 출력 값을 내놓을 때는 따로 활성화 함수를 거치지 않는다.
    - 어차피 마지막에 softmax를 통과할 것이기 때문이다.

<aside>
💡

$W_x, W_h, W_y, \vec b, \vec b_y$는 시간에 따라 변하지 않는다. 고정되는 값이다.

에포크마다 역전파를 통해서 업데이트는 되지만 데이터가 들어갈 때마다 바뀌지는 않는다.

</aside>

### RNN 표현 방법

![image.png](/assets/images/dl-theory/image%203%201.png)