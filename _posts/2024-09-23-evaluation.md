---
layout: single
title:  "Evaluation in NLP"
date:   2024-09-23 21:10:54 
categories: [ml, transformer]
author_profile: false
sidebar:
  nav: "ml"
---

## Evaluation

### Accuracy를 사용하면 안되는 이유

- 예를 들어 정답 문장이 ‘우리는 식당에서 맛있는 음식을 먹었다.’ 이고
추론 문장이 ‘식당에서 맛있는 음식을 먹었다.’ 라면 Accuracy 측면에서는 정확도가 0%이다.
- 정답과 추론이 의미상 많이 비슷한데 평가 지표가 0이라면 문제가 될 것이다.
- 그렇기에 번역 문제에 있어서는 다른 평가지표들을 사용한다.

## Perplexity (PPL)

### 정의

- N개의 토큰으로 이루어진 문장에 대한 PPL은 다음과 같다.
    - $\text{정답 문장에 대한 확률}^{-\frac{1}{N}} = P(w_1,w_2,w_3,...,w_N)^{-\frac{1}{N}} \\ = (P(w_1)\prod\limits_{i=2}^{N}P(w_i|w_1,w_2,...,w_{i-1}))^{-\frac{1}{N}}$
- 작을수록 좋은 평가지표이다.
- 헷갈리는 정도를 표현한 평가지표라고 생각하면 된다.
- 예를 들어 단어를 100개 중에 하나를 찍어야 되는 상황이면 $P = \dfrac{1}{100}$ 일텐데 이러면 PPL값은 100이 나온다.
- 즉, PPL값은 모델이 단어를 추론할 때 해당 점수만큼의 단어 수 중에 헷갈려했다는 뜻이다.

### Cross-Entropy와의 관계성

- NLP에서의 CE를 한번 생각해보자.
    - 내가 추론해야되는 단어가 ‘I’ 라면 이에 대한 CE Loss 값은 $-log(P(I))$ 일 것이다.
    - 그러면 다음으로 추론해야되는 단어가 ‘am’이라면 CE Loss는 $-log(P(am|I))$가 될 것이고
    다음 추론 단어가 ‘an’이면 CE Loss는 $-log(P(an|I,am))$이 될것이다.
- CE는 각 Loss에 대한 평균이기 때문에 NLP에선 다음과 같이 수식을 만들 수 있다.
    - $L = \frac{1}{N}\sum_{i}\text{CE} = \frac{1}{N}(-logP(w_i)+\sum_{i=2}-\log P(w_i|w_1,w_2,...,w_{i-1})) \\ = log(P(w_1)\prod\limits_{i=1}^NP(w_i|w_1,w_2,...,w_{i-1}))^{-\frac{1}{N}}$
- 수식이 어딘가 익숙하지 않은가? 해당 수식을 exp를 취한다면 PPL의 수식과 완전히 동일한 형태가 나온다.

## BLEU Score

- 번역에 있어서 PPL보다 더 신뢰성이 있는 평가지표가 BLEU Score다.
- BLEU → Bilingual Evaluation Understudy

### 정의 수식

- $\text{BLEU} = \text{BP} \cdot \prod\limits_{n=1}^{N}p_n^{w_n}$
    - $BP$ = Brevity penalty
    - $p_n$ = n-gram precision
    - $w_n$ = weight (합이 1)
- 0과 1 사이의 값이 나온다.

### n-gram precision

- 연속한 n개의 단어가 정답 문장에 존재하는 지에 대한 여부이다.
- 예를 들어 다음과 같은 정답과 예측이 있다고 하자.
    - 정답 : 훌륭한 강사와 훌륭한 수강생이 만나면 명강의가 탄생한다.
    - 예측 : 훌륭한 강사와 훌륭한 수강생이 함께라면 명강의가 만들어진다.
- 만약 1-gram preicision이라면 다음과 같이 계산된다.
    - 1-gram precision = $\frac{\text{정답 문장에 존재하는지 여부의 합계}}{\text{예측 문장의 1-gram 개수}}$
    - 즉, 단어 하나씩 끊어서 예측과 정답을 비교해서 몇개가 맞았는지를 보는 것이다.
        - 위치는 상관 없다.
- 그렇다면 2-gram precision은 어떻게 될까?
    - 단어 두개씩 끊어 보는 것이다. 즉, ‘훌륭한 강사와’, ‘강사와 훌륭한’, ‘훌륭한 수강생이’, ‘수강생이 함께라면’ … 이런식으로 말이다.
    - 즉, 2-gram precision = $\frac{3}{6}$ 이 나오게 된다.

### weight

- $w_n = \frac{1}{n}$으로 보통 사용된다.
- 여기서 n은 하이퍼 파라미터이며 그램 수를 뜻한다.
    - 하지만 default로 4를 사용한다.

### 왜 n-gram을 사용하나?

- 만약 Unigram만 사용했다고 가정하고 다음 예시를 봐보자.
    - 예측 : 훌륭한 강사와 훌륭한 수강생이 만나면 명강의가 탄생한다
    - 예측 : 만들어진다 강사와 훌륭한 명강의가 훌륭한 수강생이 함께라면
- 두 예측의 Precision은 동일하다. 하지만 첫번째 예측 문장이 훨씬 자연스럽다. 그렇기에 Unigram만 사용하면 판별력이 떨어진다.

### Clipping

- 다음 예시도 한번 봐보도록 하자.
    - 정답: 훌륭한 강사와 훌륭한 수강생이 만나면 명강의가 탄생한다
    - 예측: 훌륭한 강사와 훌륭한 수강생이 훌륭한 강사와 훌륭한 강의를 만든다
- 해당 예시는 Unigram 기준으로 정밀도가 $\frac{7}{9}$로 상당히 높은데 예측 문장은 매우 어색하다.
- 그렇기에 여기서 Clipping이라는 개념이 도입되는데 한마디로 해당 gram이 존재하는 개수 까지만 늘리라는 것이다.
- 정답 문장에서 ‘훌륭한’이라는 단어가 총 2번 나왔으므로 예측해서도 2번까지만 카운팅을 하는 것이다.
- Clipping을 이용하면 정밀도가 $\frac{4}{9}$로 떨어진다.

### BP (Brevity Penalty)

- BP를 직역하자면 짧음 패널티이다. 즉, 문장이 짧을수록 패널티를 주겠다는 것이다.
- 다음 예시를 봐보자.
    - 정답: 훌륭한 강사와 훌륭한 수강생이 만나면 명강의가 탄생한다
    - 예측: 수강생이 만나면 명강의가 탄생한다
- BP 없이 BLEU를 계산하면 1이 나오며 BLEU 기준 최대 점수를 주게 된다. 하지만 예측이 만점짜리는 아니지 않은가?
- 너무 간결해서 중요 정보를 빠뜨린 것을 반영하기 위해 BP는 다음과 같은 식으로 계산되어 BLEU 수식에 추가된다.
    - $BP = \begin{cases} 1 &\text{if }c \geq r\\ e^{1-r/c} &\text{if } c \leq r\end{cases}$
    - $r$ = 정답 문장 길이, $c$ = 예측 문장 길이
- 길게 번역하는건 문제 없나?
    - 길게 번역하면 어차피 n-gram precision 계산 도중 BLEU 점수가 깎이게 된다.

### 여러 문장에 대한 BLEU Score

![image 17.png](/assets/images/transformer/image%2017.png)

- 간단하게 그냥 다
- 해당 방법을 통해 문장 길이가 애초에 짧아서 4-gram이 0이 나오는 경우때문에 점수가 들쭉날쭉한걸 방지할 수 있다.
- 논문에서는 BLEU에 100을 곱해서 0~100의 범위로 평가지표를 나타낸다.